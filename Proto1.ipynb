{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-0EZfw29f4gk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "6xVu6TvaxKOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HPS:\n",
        "  def __init__(self, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "      setattr(self, key, value)"
      ],
      "metadata": {
        "id": "mTawAL-GxLg-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HPS"
      ],
      "metadata": {
        "id": "LQPX3sPcgT4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hps_config = {\n",
        "    'in_channels': [3, 16, 32, 64, 128],\n",
        "    'out_channels': [16, 32, 64, 128, 256],\n",
        "    'kernel_size': [3, 3, 3, 3, 3],\n",
        "    'dilations': [[1, 3, 9], [1, 3, 9], [1, 3, 9], [1, 3, 9], [1, 3, 9]],\n",
        "    'stride': [1, 1, 1, 1, 1],\n",
        "    'padding': ['same', 'same', 'same', 'same', 'same'],\n",
        "    'embed_dim': [16, 32, 64, 128, 256],\n",
        "    'h_patch': [8, 8, 8, 8, 8],\n",
        "    'w_patch': [8, 8, 8, 8, 8],\n",
        "    'num_heads': [8, 8, 8, 8, 8],\n",
        "    'ff_dim': [16, 32, 64, 128, 256],\n",
        "    'attn_layers': [2, 2, 2, 4, 4],\n",
        "    'mlp_in': [256, 64],\n",
        "    'mlp_out': [64, 32],\n",
        "    'dropout': 0.1,\n",
        "    'num_layers': 5,\n",
        "    'num_labels': 6\n",
        "}"
      ],
      "metadata": {
        "id": "o962Gu8qgWIm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convolution Block"
      ],
      "metadata": {
        "id": "6D-U4n1dgXhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dilation=1, stride=1, padding='same', dropout=0):\n",
        "    super(ConvolutionBlock, self).__init__()\n",
        "\n",
        "    self.expansion = nn.Conv2d(in_channels, 2 * in_channels, kernel_size=1)\n",
        "\n",
        "    self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=in_channels)\n",
        "    self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    self.batch_norm1 = nn.BatchNorm2d(in_channels)\n",
        "    self.batch_norm2 = nn.BatchNorm2d(in_channels)\n",
        "    self.glu = nn.GLU(dim=1)\n",
        "    self.swish = nn.SiLU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    x = self.batch_norm1(x)\n",
        "    x = self.expansion(x)\n",
        "    x = self.glu(x)\n",
        "\n",
        "    x = self.depthwise(x)\n",
        "    x = self.batch_norm2(x)\n",
        "    x = self.swish(x)\n",
        "\n",
        "    x = self.pointwise(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "iTid7GQAgZSL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pooling Attention"
      ],
      "metadata": {
        "id": "6MjekJILiOZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoolingAttention(nn.Module):\n",
        "  def __init__(self, h_patch, w_patch, embed_dim, ff_dim, num_heads, dropout):\n",
        "    super(PoolingAttention, self).__init__()\n",
        "\n",
        "    self.h_patch = h_patch\n",
        "    self.w_patch = w_patch\n",
        "\n",
        "    self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout)\n",
        "    self.linear = nn.Linear(embed_dim, ff_dim)\n",
        "    self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    self.swish = nn.SiLU()\n",
        "\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, h_patch * w_patch, embed_dim))\n",
        "    nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    assert H % self.h_patch == 0 and W % self.w_patch == 0, \"H and W must be divisible by h_patch and w_patch\"\n",
        "\n",
        "    H_patch = H // self.h_patch\n",
        "    W_patch = W // self.w_patch\n",
        "\n",
        "    N = self.h_patch * self.w_patch\n",
        "\n",
        "    x = x.view(B, C, self.h_patch, H_patch, self.w_patch, W_patch)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5)\n",
        "    x = x.reshape(B, N, C, H_patch, W_patch)\n",
        "\n",
        "    x = x.view(-1, C, H_patch, W_patch)\n",
        "    x = F.avg_pool2d(x, kernel_size=(H_patch, W_patch))\n",
        "    x = x.view(B, N, C)\n",
        "\n",
        "    x = x + self.pos_embed\n",
        "\n",
        "    x = self.norm1(x)\n",
        "    _x, _ = self.attention(x, x, x)\n",
        "    x = x + self.dropout1(_x)\n",
        "\n",
        "    x = self.norm2(x)\n",
        "    _x = self.linear(x)\n",
        "    _x = self.swish(_x)\n",
        "    _x = self.linear2(_x)\n",
        "    x = x + self.dropout2(_x)\n",
        "\n",
        "    x = torch.sigmoid(x)\n",
        "\n",
        "    x_out = x.view(B, self.h_patch, self.w_patch, C)\n",
        "    x_out = x_out.permute(0, 3, 1, 2)\n",
        "\n",
        "    x_out = x_out.repeat_interleave(H_patch, dim=2).repeat_interleave(W_patch, dim=3)\n",
        "\n",
        "    return x_out\n"
      ],
      "metadata": {
        "id": "WLB_R4FwiRzw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Residual Block"
      ],
      "metadata": {
        "id": "BuhZisR2r1T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dilations, stride=1, padding='same', dropout=0):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.residual_blocks = nn.ModuleList([\n",
        "        ConvolutionBlock(in_channels, in_channels, kernel_size, dilation, stride, padding, dropout)\n",
        "        for dilation in dilations\n",
        "    ])\n",
        "\n",
        "    self.conv_block = ConvolutionBlock(in_channels, out_channels, kernel_size, dilation=1, stride=1, padding='same', dropout=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for residual_block in self.residual_blocks:\n",
        "      x = x + residual_block(x)\n",
        "\n",
        "    return self.conv_block(x)"
      ],
      "metadata": {
        "id": "vjlxoTDnphW9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EncoderLayer"
      ],
      "metadata": {
        "id": "-AoFD3WosPmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels,\n",
        "               out_channels,\n",
        "               kernel_size,\n",
        "               dilations,\n",
        "               stride,\n",
        "               padding,\n",
        "               embed_dim,\n",
        "               h_patch,\n",
        "               w_patch,\n",
        "               num_heads,\n",
        "               ff_dim,\n",
        "               attn_layers,\n",
        "               dropout=0):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.residual_block = ResidualBlock(in_channels, out_channels, kernel_size, dilations, stride, padding, dropout)\n",
        "\n",
        "    self.attns = nn.ModuleList([\n",
        "        PoolingAttention(h_patch, w_patch, embed_dim, ff_dim, num_heads, dropout)\n",
        "        for _ in range(attn_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.residual_block(x)\n",
        "\n",
        "    for attn in self.attns:\n",
        "      _x = attn(x)\n",
        "      x = x * _x\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "55rhpsdxsSOD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "yoCl_GwBxftp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, hps):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "        EncoderLayer(\n",
        "            hps.in_channels[i],\n",
        "            hps.out_channels[i],\n",
        "            hps.kernel_size[i],\n",
        "            hps.dilations[i],\n",
        "            hps.stride[i],\n",
        "            hps.padding[i],\n",
        "            hps.embed_dim[i],\n",
        "            hps.h_patch[i],\n",
        "            hps.w_patch[i],\n",
        "            hps.num_heads[i],\n",
        "            hps.ff_dim[i],\n",
        "            hps.attn_layers[i],\n",
        "            hps.dropout\n",
        "        )\n",
        "        for i in range(hps.num_layers)\n",
        "    ])\n",
        "\n",
        "    self.mlps = nn.ModuleList([\n",
        "        nn.Sequential(\n",
        "          nn.Linear(hps.mlp_in[i], hps.mlp_out[i]),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(hps.dropout)\n",
        "        )\n",
        "        for i in range(len(hps.mlp_in))\n",
        "    ])\n",
        "\n",
        "    self.classifier = nn.Linear(hps.mlp_out[-1], hps.num_labels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x)\n",
        "\n",
        "    x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "    for mlp in self.mlps:\n",
        "      x = mlp(x)\n",
        "\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    x = F.softmax(x, dim=-1)\n",
        "\n",
        "    return x.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "xPZwC2AquefK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Initialization"
      ],
      "metadata": {
        "id": "snpsKc_wxQxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hps = HPS(**hps_config)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = Model(hps).to(device)"
      ],
      "metadata": {
        "id": "UOSTbLwwvuX4"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}