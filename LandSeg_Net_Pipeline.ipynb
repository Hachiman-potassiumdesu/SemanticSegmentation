{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3I5juE8WHDB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-NI5Ipg-yQD"
      },
      "source": [
        "## Dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77EtTYuF-P59"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY_Y-oZ7-sy1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njQXG-v_z-Iq"
      },
      "source": [
        "## Model Initialization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeUSwGfL-3GT"
      },
      "source": [
        "## Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT-qfut--2c6"
      },
      "outputs": [],
      "source": [
        "# Separate configuration and model variables.\n",
        "\n",
        "config_vars = {\n",
        "    # 'kaggle_token': [], # In [USER, PASS] format.\n",
        "    'drive_path': '',\n",
        "    'dataset_path': 'kevingeng07/landseg-net-3-country-clc-data/versions/1',\n",
        "    'save_path': '', # Google Drive directory to save the file.\n",
        "    'train_bs': 256,\n",
        "    'val_bs': 256,\n",
        "    'test_bs': 512,\n",
        "    'device': 'cuda',\n",
        "    'loss_fn': torch.nn.CrossEntropyLoss(),\n",
        "    'epochs': 100,\n",
        "    'model': model, # Add the initialized model.\n",
        "    'optimizer': torch.optim.AdamW,\n",
        "    'optimizer_params': {'lr': 1e-3, 'weight_decay': 1e-2},\n",
        "    'scheduler_params': {\n",
        "        'schedulers': [\n",
        "            (lr_scheduler.LinearLR, {'start_factor': 5e-5/1e-3, 'total_iters': 5}),\n",
        "            (lr_scheduler.CosineAnnealingLR, {'T_max': 95})\n",
        "        ],\n",
        "        'milestones': [5]\n",
        "    },\n",
        "    'patience': 10\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WXNdOkz-rTk"
      },
      "source": [
        "## Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKBkRYA4-qdo"
      },
      "outputs": [],
      "source": [
        "### TODO: CREATE NEW CONFIG VARIABLE \"drive_path\" FOR THE PARENT DIRECTORY OF THE DATA.\n",
        "\n",
        "def create_dataset(**kwargs):\n",
        "  # kaggle_token = kwargs.get('kaggle_token')\n",
        "  drive_path = kwargs.get('drive_path') # Path to drive folder that contains the dataset, not the dataset folder itself.\n",
        "  dataset_path = kwargs.get('dataset_path')\n",
        "\n",
        "  # os.environ['KAGGLE_USERNAME'] = kaggle_token[0]\n",
        "  # os.environ['KAGGLE_KEY'] = kaggle_token[1]\n",
        "\n",
        "  # kagglehub.login()\n",
        "  # kagglehub.dataset_download(dataset_path)\n",
        "\n",
        "\n",
        "  class DatasetGenerator(utils.Dataset):\n",
        "    def __init__(self, split, drive_path, dataset_path):\n",
        "\n",
        "      self.split = split\n",
        "\n",
        "      self.data_dir = f'/content/{dataset_path}'\n",
        "      self.data_paths = []\n",
        "\n",
        "      shutil.copytree(f'/content/drive/{drive_path}/{dataset_path}', self.data_dir)\n",
        "      for file_path in os.listdir(os.path.join(self.data_dir, split)):\n",
        "        self.data_paths.append(os.path.join(self.data_dir, split, file_path))\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      data = np.load(self.data_paths[idx])\n",
        "\n",
        "      if self.split == 'train':\n",
        "        data = self.augment_data(data)\n",
        "\n",
        "      features = data[:-1, :, :].astype(np.float32) / 10000 # Correctly scale the data according to the raw reflectance values.\n",
        "      labels = data[-1, :, :].astype(np.uint16)\n",
        "\n",
        "      labels[np.isin(labels, [111, 112, 121, 122, 123, 124, 131, 132, 133, 141, 142, 211, 212, 213, 221, 222, 223, 231, 241, 242, 243, 244, 323, 331, 332, 334, 421, 422, 423, 521, 522, 523, 999])] = 0\n",
        "\n",
        "      # Natural land cover.\n",
        "      unique_classes = [0, 311, 312, 313, 321, 322, 324, 333, 411, 412, 511, 512]\n",
        "      mapping = {v: i for i, v in enumerate(unique_classes)}\n",
        "      max_class_value = unique_classes.max()\n",
        "      lookup = np.full(max_class_value + 1, -1, dtype=int)\n",
        "      for k, v in mapping.items():\n",
        "          lookup[k] = v\n",
        "      labels = lookup[labels]\n",
        "\n",
        "      return features, labels\n",
        "\n",
        "    def augment_data(self, x):\n",
        "      transforms_list = []\n",
        "\n",
        "      if np.random.random() <= 0.5:\n",
        "        transforms_list.append(v2.RandomHorizontalFlip(p=1.0))\n",
        "\n",
        "      if np.random.random() <= 0.5:\n",
        "        transforms_list.append(v2.RandomVerticalFlip(p=1.0))\n",
        "\n",
        "      if np.random.random() <= 0.25:\n",
        "        transforms_list.append(v2.RandomRotation(degrees=135))\n",
        "\n",
        "      if np.random.random() <= 0.15:\n",
        "        transforms_list.append(v2.GaussianNoise(sigma=0.15, clip=True)) # Reduce simga to 0.05 first if underfitting, or try different sigmas based on band mean (ie. B01-02 has lower values for some samples).\n",
        "\n",
        "      return v2.Compose(transforms_list)(x)\n",
        "\n",
        "\n",
        "  train_set = DatasetGenerator('train')\n",
        "  validation_set = DatasetGenerator('validation')\n",
        "  test_set = DatasetGenerator('test')\n",
        "\n",
        "  return utils.DataLoader(train_set, batch_size=kwargs.get('train_bs'), shuffle=True, drop_last=True, num_workers=8, persistent_workers=True, pin_memory=True),\n",
        "  utils.DataLoader(validation_set, batch_size=kwargs.get('val_bs'), shuffle=False, drop_last=True, num_workers=8, persistent_workers=True, pin_memory=True),\n",
        "  utils.DataLoader(test_set, batch_size=kwargs.get('test_bs'), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo8g04t4-xaS"
      },
      "source": [
        "## Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkpJ1hEVxb4y"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, tol=0.01):\n",
        "    self.patience = patience\n",
        "    self.tol = tol\n",
        "\n",
        "    self.cur_epoch = 0\n",
        "    self.lowest_val_loss = float(0)\n",
        "    self.best_model = None\n",
        "\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if (val_loss <= self.lowest_val_loss-self.tol or self.lowest_val_loss == 0):\n",
        "      self.cur_epoch = 0\n",
        "      self.best_model = model.state_dict()\n",
        "      self.lowest_val_loss = val_loss\n",
        "\n",
        "    if self.cur_epoch == self.patience:\n",
        "      return self.best_model\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hifs61J--7SK"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, val_loader, **kwargs):\n",
        "  assert kwargs.get('optimizer') # Either a LR scheduler, or optimizer, must exist for the model.\n",
        "\n",
        "  device = kwargs.get('device')\n",
        "  patience = kwargs.get('patience')\n",
        "  save_path = kwargs.get('save_path')\n",
        "\n",
        "\n",
        "  model = kwargs.get('model').to(device)\n",
        "  model.init_weights()\n",
        "\n",
        "\n",
        "  optimizer = kwargs.get('optimizer')(model.parameters(), **kwargs.get('optimizer_params'))\n",
        "  if scheduler:\n",
        "    scheduler = lr_scheduler.SequentialLR(optimizer, **kwargs.get('scheduler_params'))\n",
        "\n",
        "  loss_fn = kwargs.get('loss_fn')\n",
        "\n",
        "  if patience:\n",
        "    early_stop = EarlyStopping(patience=patience)\n",
        "\n",
        "  for epoch in range(1, kwargs.get('epochs')+1):\n",
        "    train_loss = float(0)\n",
        "\n",
        "    model.train(mode=True)\n",
        "    for data in train_loader:\n",
        "      input_data, label = data\n",
        "      input_data = input_data.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      with torch.autocast(device, dtype=torch.bfloat16):\n",
        "        pred_label = model(input_data)\n",
        "        loss = loss_fn(pred_label, label)\n",
        "\n",
        "      train_loss += loss.detach().item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if (epoch - 1) % 5 == 0:\n",
        "      torch.save(model.state_dict(), f'{save_path}/epoch_{epoch}.pth')\n",
        "\n",
        "\n",
        "    model.train(mode=False)\n",
        "    with torch.zero_grad():\n",
        "      val_loss = float(0)\n",
        "\n",
        "      for data in val_loader:\n",
        "        input_data, label = data\n",
        "        input_data_val = input_data.to(device)\n",
        "        label_val = label.to(device)\n",
        "\n",
        "        pred_label_val = model(input_data_val)\n",
        "        loss = loss_fn(pred_label_val, label_val)\n",
        "\n",
        "        val_loss += loss.detach().item()\n",
        "\n",
        "      train_loss /= len(train_loader)\n",
        "      val_loss /= len(val_loader)\n",
        "\n",
        "      if patience:\n",
        "        es_result = early_stop(val_loss)\n",
        "        if es_result:\n",
        "          torch.save(es_result, f'{save_path}/BEST_epoch_{epoch-patience}.pth')\n",
        "          break # Break after early stopping is applied.\n",
        "\n",
        "    if scheduler:\n",
        "      scheduler.step()\n",
        "\n",
        "  print(f'Epoch {epoch}:\\n Train Loss: {train_loss}\\n Val Loss: {val_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OneDd0TqqqL"
      },
      "source": [
        "## Main:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA-3VQLMsoWV"
      },
      "outputs": [],
      "source": [
        "def training_pipeline(**kwargs):\n",
        "  train_data, val_data, test_data = create_dataset(**kwargs)\n",
        "  train_model(train_data, val_data, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayyO_Tkgqs6z"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  training_pipeline(**config_vars)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SeUSwGfL-3GT",
        "Xo8g04t4-xaS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
