{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u-NI5Ipg-yQD",
        "3WXNdOkz-rTk"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "D3I5juE8WHDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3a305f-26eb-447e-a751-729728385e94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies:"
      ],
      "metadata": {
        "id": "u-NI5Ipg-yQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision numpy torchinfo"
      ],
      "metadata": {
        "id": "77EtTYuF-P59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7029655-3cbb-466b-989c-e07d90887c36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as utils\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchvision.transforms import v2\n",
        "import torchinfo\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil"
      ],
      "metadata": {
        "id": "EY_Y-oZ7-sy1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Initialization:"
      ],
      "metadata": {
        "id": "njQXG-v_z-Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Light U-Net:"
      ],
      "metadata": {
        "id": "OJdM6qO1qZnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureConvs(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, padding, dropout=0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_convs = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "        nn.BatchNorm2d(num_features=in_channels),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Dropout(p=dropout),\n",
        "\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "        nn.BatchNorm2d(num_features=out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x): return self.feature_convs(x)\n",
        "\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mlp_block = nn.Sequential(\n",
        "        nn.Linear(in_features=in_dim, out_features=out_dim, bias=False),\n",
        "        nn.LayerNorm(normalized_shape=out_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x): return self.mlp_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, kwargs):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "\n",
        "    self.in_channels = kwargs.get('in_channels')[layer]\n",
        "    self.out_channels = kwargs.get('out_channels')[layer]\n",
        "    self.kernel_size = kwargs.get('kernel_size')\n",
        "    self.padding = kwargs.get('padding')\n",
        "    self.dropout = kwargs.get('dropout')\n",
        "\n",
        "    self.pool_stride = kwargs.get('pool_stride')\n",
        "\n",
        "    self.feature_convs = FeatureConvs(self.in_channels, self.out_channels, self.kernel_size, self.padding, self.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.layer != 0:\n",
        "      x = nn.functional.max_pool2d(x, kernel_size=self.pool_stride)\n",
        "\n",
        "    return self.feature_convs(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layer, kwargs):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "    self.layers = kwargs.get('layers')\n",
        "\n",
        "    self.in_channels = kwargs.get('out_channels')[layer]\n",
        "    self.out_channels = kwargs.get('in_channels')[layer]\n",
        "    self.kernel_size = kwargs.get('kernel_size')\n",
        "    self.padding = kwargs.get('padding')\n",
        "\n",
        "    self.pool_stride = kwargs.get('pool_stride')\n",
        "    self.pointwise = nn.Conv2d(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1)\n",
        "\n",
        "    self.feature_convs = FeatureConvs(self.in_channels, self.out_channels, self.kernel_size, self.padding)\n",
        "\n",
        "  def forward(self, x, _x=None):\n",
        "    x = self.pointwise(nn.functional.interpolate(x, scale_factor=self.pool_stride))\n",
        "    x += _x\n",
        "\n",
        "    return self.feature_convs(x)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  def __init__(self, kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dim_list = kwargs.get('mlp_dims')\n",
        "\n",
        "    self.mlp = nn.ModuleList([MLPBlock(i, o) for i, o in zip(self.dim_list[:-1], self.dim_list[1:])])\n",
        "\n",
        "    self.process_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=kwargs.get('out_channels')[-1], out_channels=kwargs.get('in_channels')[-1], kernel_size=kwargs.get('kernel_size'), padding=kwargs.get('padding'), bias=False),\n",
        "        nn.BatchNorm2d(num_features=kwargs.get('in_channels')[-1]),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "    for i in self.mlp:\n",
        "      x = i(x)\n",
        "\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    return self.process_conv(x)"
      ],
      "metadata": {
        "id": "AlTWbcvmqcGf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Light_UNet(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.enc_1 = Encoder(0, kwargs)\n",
        "    self.enc_2 = Encoder(1, kwargs)\n",
        "    self.enc_3 = Encoder(2, kwargs)\n",
        "    self.enc_4 = Encoder(3, kwargs)\n",
        "\n",
        "    self.dec_3 = Decoder(2, kwargs)\n",
        "    self.dec_2 = Decoder(1, kwargs)\n",
        "    self.dec_1 = Decoder(0, kwargs)\n",
        "\n",
        "    self.bottleneck = Bottleneck(kwargs)\n",
        "\n",
        "    self.first_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=kwargs.get('feature_labels')[0], out_channels=kwargs.get('in_channels')[0], kernel_size=kwargs.get('kernel_size'), padding=kwargs.get('padding'), bias=False),\n",
        "        nn.BatchNorm2d(num_features=kwargs.get('in_channels')[0]),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.last_conv = nn.Conv2d(in_channels=kwargs.get('out_channels')[0], out_channels=kwargs.get('feature_labels')[-1], kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_1 = self.enc_1(self.first_conv(x))\n",
        "    x_2 = self.enc_2(x_1)\n",
        "    x_3 = self.enc_3(x_2)\n",
        "    x_4 = self.bottleneck(self.enc_4(x_3))\n",
        "\n",
        "    x_3 = self.dec_3(x_4, x_3)\n",
        "    x_2 = self.dec_2(x_3, x_2)\n",
        "    x_1 = self.dec_1(x_2, x_1)\n",
        "\n",
        "    return self.last_conv(x_1)\n",
        "\n",
        "  def init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')"
      ],
      "metadata": {
        "id": "QDyZlfUcDfLL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    'layers': 4,\n",
        "    'kernel_size': 3,\n",
        "    'padding': 'same',\n",
        "    'dropout': 0.1,\n",
        "    'pool_stride': 2,\n",
        "    'feature_labels': [12, 20], # Adjust element 1 based on number of valid labels for task.\n",
        "    'in_channels': [64, 64, 128, 256],\n",
        "    'out_channels': [64, 128, 256, 512],\n",
        "    'mlp_dims': [512, 1024, 1024, 512]\n",
        "}"
      ],
      "metadata": {
        "id": "_3mW8yG5N_eL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(Light_UNet(**model_config), input_size=((1, 12, 120, 120)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3suHBSihYTaW",
        "outputId": "b254de4e-0cd1-43c2-d2f7-2bf5148a203e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Light_UNet                               [1, 20, 120, 120]         --\n",
              "├─Sequential: 1-1                        [1, 64, 120, 120]         --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 120, 120]         6,912\n",
              "│    └─BatchNorm2d: 2-2                  [1, 64, 120, 120]         128\n",
              "│    └─ReLU: 2-3                         [1, 64, 120, 120]         --\n",
              "├─Encoder: 1-2                           [1, 64, 120, 120]         --\n",
              "│    └─FeatureConvs: 2-4                 [1, 64, 120, 120]         --\n",
              "│    │    └─Sequential: 3-1              [1, 64, 120, 120]         73,984\n",
              "├─Encoder: 1-3                           [1, 128, 60, 60]          --\n",
              "│    └─FeatureConvs: 2-5                 [1, 128, 60, 60]          --\n",
              "│    │    └─Sequential: 3-2              [1, 128, 60, 60]          110,976\n",
              "├─Encoder: 1-4                           [1, 256, 30, 30]          --\n",
              "│    └─FeatureConvs: 2-6                 [1, 256, 30, 30]          --\n",
              "│    │    └─Sequential: 3-3              [1, 256, 30, 30]          443,136\n",
              "├─Encoder: 1-5                           [1, 512, 15, 15]          --\n",
              "│    └─FeatureConvs: 2-7                 [1, 512, 15, 15]          --\n",
              "│    │    └─Sequential: 3-4              [1, 512, 15, 15]          1,771,008\n",
              "├─Bottleneck: 1-6                        [1, 256, 15, 15]          --\n",
              "│    └─ModuleList: 2-8                   --                        --\n",
              "│    │    └─MLPBlock: 3-5                [1, 15, 15, 1024]         526,336\n",
              "│    │    └─MLPBlock: 3-6                [1, 15, 15, 1024]         1,050,624\n",
              "│    │    └─MLPBlock: 3-7                [1, 15, 15, 512]          525,312\n",
              "│    └─Sequential: 2-9                   [1, 256, 15, 15]          --\n",
              "│    │    └─Conv2d: 3-8                  [1, 256, 15, 15]          1,179,648\n",
              "│    │    └─BatchNorm2d: 3-9             [1, 256, 15, 15]          512\n",
              "│    │    └─ReLU: 3-10                   [1, 256, 15, 15]          --\n",
              "├─Decoder: 1-7                           [1, 128, 30, 30]          --\n",
              "│    └─Conv2d: 2-10                      [1, 256, 30, 30]          65,792\n",
              "│    └─FeatureConvs: 2-11                [1, 128, 30, 30]          --\n",
              "│    │    └─Sequential: 3-11             [1, 128, 30, 30]          885,504\n",
              "├─Decoder: 1-8                           [1, 64, 60, 60]           --\n",
              "│    └─Conv2d: 2-12                      [1, 128, 60, 60]          16,512\n",
              "│    └─FeatureConvs: 2-13                [1, 64, 60, 60]           --\n",
              "│    │    └─Sequential: 3-12             [1, 64, 60, 60]           221,568\n",
              "├─Decoder: 1-9                           [1, 64, 120, 120]         --\n",
              "│    └─Conv2d: 2-14                      [1, 64, 120, 120]         4,160\n",
              "│    └─FeatureConvs: 2-15                [1, 64, 120, 120]         --\n",
              "│    │    └─Sequential: 3-13             [1, 64, 120, 120]         73,984\n",
              "├─Conv2d: 1-10                           [1, 20, 120, 120]         1,300\n",
              "==========================================================================================\n",
              "Total params: 6,957,396\n",
              "Trainable params: 6,957,396\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 5.47\n",
              "==========================================================================================\n",
              "Input size (MB): 0.69\n",
              "Forward/backward pass size (MB): 135.01\n",
              "Params size (MB): 27.83\n",
              "Estimated Total Size (MB): 163.54\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration:"
      ],
      "metadata": {
        "id": "SeUSwGfL-3GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate configuration and model variables.\n",
        "\n",
        "config_vars = {\n",
        "    # 'kaggle_token': [], # In [USER, PASS] format.\n",
        "    'drive_path': 'MyDrive/Semantic Segmentation/data/new_natural_ecosystems',\n",
        "    'save_path': 'MyDrive/Semantic Segmentation/light u-net chpts', # Google Drive directory to save the file.\n",
        "    'train_bs': 256,\n",
        "    'val_bs': 256,\n",
        "    'test_bs': 512,\n",
        "    'device': 'cuda',\n",
        "    'loss_fn': torch.nn.CrossEntropyLoss(),\n",
        "    'epochs': 100,\n",
        "    'model': Light_UNet(**model_config), # Add the initialized model.\n",
        "    'optimizer': torch.optim.AdamW,\n",
        "    'optimizer_params': {'lr': 1e-3, 'weight_decay': 1e-2},\n",
        "    'scheduler_params': {\n",
        "        'schedulers': [\n",
        "            (lr_scheduler.LinearLR, {'start_factor': 5e-5/1e-3, 'total_iters': 5}),\n",
        "            (lr_scheduler.CosineAnnealingLR, {'T_max': 95})\n",
        "        ],\n",
        "        'milestones': [5]\n",
        "    },\n",
        "    'patience': 10\n",
        "}"
      ],
      "metadata": {
        "id": "jT-qfut--2c6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset:"
      ],
      "metadata": {
        "id": "3WXNdOkz-rTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aKBkRYA4-qdo"
      },
      "outputs": [],
      "source": [
        "def create_dataset(**kwargs):\n",
        "  drive_path = kwargs.get('drive_path') # Path to dataset folder.\n",
        "\n",
        "  class DatasetGenerator(utils.Dataset):\n",
        "    def __init__(self, split):\n",
        "\n",
        "      self.split = split\n",
        "\n",
        "      shutil.copyfile(f'/content/drive/{drive_path}/{split}.npy', f'/content/{split}.npy')\n",
        "      self.data = np.load(f'/content/{split}.npy')\n",
        "\n",
        "      # Natural land cover.\n",
        "      unique_classes = [0, 311, 312, 313, 321, 322, 324, 333, 411, 412, 511, 512]\n",
        "      mapping = {v: i for i, v in enumerate(unique_classes)}\n",
        "      max_class_value = max(unique_classes)\n",
        "      self.lookup = np.full(max_class_value + 1, -1, dtype=int)\n",
        "      for k, v in mapping.items():\n",
        "          self.lookup[k] = v\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      data = self.data[idx]\n",
        "\n",
        "      if self.split == 'train':\n",
        "        data = self.augment_data(data)\n",
        "\n",
        "      features = data[:-1, :, :].astype(np.float32) / 10000 # Correctly scale the data according to the raw reflectance values.\n",
        "      labels = data[-1, :, :].astype(np.uint16)\n",
        "\n",
        "      labels[np.isin(labels, [111, 112, 121, 122, 123, 124, 131, 132, 133, 141, 142, 211, 212, 213, 221, 222, 223, 231, 241, 242, 243, 244, 323, 331, 332, 334, 421, 422, 423, 521, 522, 523, 999])] = 0\n",
        "      labels = self.lookup[labels]\n",
        "\n",
        "      return features, labels\n",
        "\n",
        "    def augment_data(self, x):\n",
        "      transforms_list = []\n",
        "\n",
        "      if np.random.random() <= 0.5:\n",
        "        transforms_list.append(v2.RandomHorizontalFlip(p=1.0))\n",
        "\n",
        "      if np.random.random() <= 0.5:\n",
        "        transforms_list.append(v2.RandomVerticalFlip(p=1.0))\n",
        "\n",
        "      if np.random.random() <= 0.25:\n",
        "        transforms_list.append(v2.RandomRotation(degrees=135))\n",
        "\n",
        "      if np.random.random() <= 0.15:\n",
        "        transforms_list.append(v2.GaussianNoise(sigma=0.15, clip=True)) # Reduce simga to 0.05 first if underfitting, or try different sigmas based on band mean (ie. B01-02 has lower values for some samples).\n",
        "\n",
        "      if transforms_list:\n",
        "        return v2.Compose(transforms_list)(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  train_set = DatasetGenerator('train')\n",
        "  print('Train Copied')\n",
        "  validation_set = DatasetGenerator('validation')\n",
        "  print('Validation Copied')\n",
        "  test_set = DatasetGenerator('test')\n",
        "  print('Test Copied')\n",
        "\n",
        "  return (utils.DataLoader(train_set, batch_size=kwargs.get('train_bs'), shuffle=True, drop_last=True, num_workers=8, persistent_workers=True, pin_memory=True),\n",
        "          utils.DataLoader(validation_set, batch_size=kwargs.get('val_bs'), shuffle=False, drop_last=True, num_workers=8, persistent_workers=True, pin_memory=True),\n",
        "          utils.DataLoader(test_set, batch_size=kwargs.get('test_bs'), shuffle=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training:"
      ],
      "metadata": {
        "id": "Xo8g04t4-xaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, tol=0.01):\n",
        "    self.patience = patience\n",
        "    self.tol = tol\n",
        "\n",
        "    self.cur_epoch = 0\n",
        "    self.lowest_val_loss = float(0)\n",
        "    self.best_model = None\n",
        "\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if (val_loss <= self.lowest_val_loss-self.tol or self.lowest_val_loss == 0):\n",
        "      self.cur_epoch = 0\n",
        "      self.best_model = model.state_dict()\n",
        "      self.lowest_val_loss = val_loss\n",
        "    else:\n",
        "      self.cur_epoch += 1 # Increment counter if no improvement\n",
        "\n",
        "    if self.cur_epoch == self.patience:\n",
        "      return self.best_model\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "EkpJ1hEVxb4y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, val_loader, **kwargs):\n",
        "  device = kwargs.get('device')\n",
        "  patience = kwargs.get('patience')\n",
        "  save_path = kwargs.get('save_path')\n",
        "\n",
        "\n",
        "  model = kwargs.get('model').to(device)\n",
        "  model.init_weights()\n",
        "\n",
        "\n",
        "  optimizer = kwargs.get('optimizer')(model.parameters(), **kwargs.get('optimizer_params'))\n",
        "  scheduler = None\n",
        "  if kwargs.get('scheduler_params'):\n",
        "    scheduler_details = kwargs.get('scheduler_params')\n",
        "    schedulers_list = [scheduler_class(optimizer, **params) for scheduler_class, params in scheduler_details.get('schedulers')]\n",
        "    scheduler = lr_scheduler.SequentialLR(optimizer, schedulers=schedulers_list, milestones=scheduler_details.get('milestones'))\n",
        "\n",
        "  loss_fn = kwargs.get('loss_fn')\n",
        "\n",
        "  if patience:\n",
        "    early_stop = EarlyStopping(patience=patience)\n",
        "\n",
        "  for epoch in range(1, kwargs.get('epochs')+1):\n",
        "    train_loss = float(0)\n",
        "\n",
        "    model.train(mode=True)\n",
        "    for data in train_loader:\n",
        "      input_data, label = data\n",
        "      input_data = input_data.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      # for ch in range(12):\n",
        "      #   print(ch, input_data[0, ch, ...].mean())\n",
        "\n",
        "      # print(label[0, ...].unique())\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      with torch.autocast(device, dtype=torch.bfloat16):\n",
        "        pred_label = model(input_data)\n",
        "        loss = loss_fn(pred_label, label)\n",
        "\n",
        "      train_loss += loss.detach().item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if (epoch - 1) % 5 == 0:\n",
        "      torch.save(model.state_dict(), f'/content/drive/{save_path}/epoch_{epoch}.pth')\n",
        "\n",
        "\n",
        "    model.train(mode=False)\n",
        "    with torch.no_grad():\n",
        "      val_loss = float(0)\n",
        "\n",
        "      for data in val_loader:\n",
        "        input_data, label = data\n",
        "        input_data_val = input_data.to(device)\n",
        "        label_val = label.to(device)\n",
        "\n",
        "        pred_label_val = model(input_data_val)\n",
        "        loss = loss_fn(pred_label_val, label_val)\n",
        "\n",
        "        val_loss += loss.detach().item()\n",
        "\n",
        "      train_loss /= len(train_loader)\n",
        "      val_loss /= len(val_loader)\n",
        "\n",
        "      if patience and early_stop:\n",
        "        es_result = early_stop(val_loss, model)\n",
        "        if es_result:\n",
        "          torch.save(es_result, f'/content/drive/{save_path}/BEST_epoch_{epoch-patience}.pth')\n",
        "          break\n",
        "\n",
        "    if scheduler:\n",
        "      scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch}:\\n Train Loss: {train_loss}\\n Val Loss: {val_loss}')"
      ],
      "metadata": {
        "id": "Hifs61J--7SK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main:"
      ],
      "metadata": {
        "id": "2OneDd0TqqqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(config_vars.get('drive_path'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDBGhi0oIhfb",
        "outputId": "5fc164cd-bc69-478d-dfed-9ea877e2c77b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive/Semantic Segmentation/data/new_natural_ecosystems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training_pipeline(**kwargs):\n",
        "  # Set seeds for reproducibility.\n",
        "\n",
        "\n",
        "  train_data, val_data, test_data = create_dataset(**kwargs)\n",
        "  train_model(train_data, val_data, **kwargs)"
      ],
      "metadata": {
        "id": "qA-3VQLMsoWV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  training_pipeline(**config_vars)"
      ],
      "metadata": {
        "id": "ayyO_Tkgqs6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea71615-0055-4c30-f3fc-368098b3f936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Copied\n",
            "Validation Copied\n",
            "Test Copied\n",
            "Epoch 1:\n",
            " Train Loss: 1.8568679365781273\n",
            " Val Loss: 1.975659940553748\n",
            "Epoch 2:\n",
            " Train Loss: 0.9423453129164063\n",
            " Val Loss: 0.8115009833937106\n",
            "Epoch 3:\n",
            " Train Loss: 0.7131560102547749\n",
            " Val Loss: 0.7660000751847806\n",
            "Epoch 4:\n",
            " Train Loss: 0.6290817803675586\n",
            " Val Loss: 0.7815427650576052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5:\n",
            " Train Loss: 0.5819325718549219\n",
            " Val Loss: 0.8112700095643168\n"
          ]
        }
      ]
    }
  ]
}