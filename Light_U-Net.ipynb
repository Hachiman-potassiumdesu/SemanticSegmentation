## Light U-Net:

class FeatureConvs(nn.Module):
  def __init__(self, layer, in_channels, out_channels, kernel_size, padding, dropout=0.0):
    super().__init__()

    self.feature_convs = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, padding=padding, bias=False),
        nn.BatchNorm2d(num_features=in_channels),
        nn.ReLU(),

        nn.Dropout(p=dropout),

        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, bias=False),
        nn.BatchNorm2d(num_features=out_channels),
        nn.ReLU()
    )

  def forward(self, x): return self.feature_convs(x)


class MLPBlock(nn.Module):
  def __init__(self, in_dim, out_dim):
    super().__init__()

    self.mlp_block = nn.Sequential(
        nn.Linear(in_features=in_dim, out_features=out_dim, bias=False),
        nn.LayerNorm(normalized_shape=out_dim),
        nn.ReLU()
    )

  def forward(self, x): return self.mlp_block(x)


class Encoder(nn.Module):
  def __init__(self, layer, kwargs):
    super().__init__()
    self.layer = layer

    self.in_channels = kwargs.get('in_channels')[layer]
    self.out_channels = kwargs.get('out_channels')[layer]
    self.kernel_size = kwargs.get('kernel_size')
    self.padding = kwargs.get('padding')
    self.dropout = kwargs.get('dropout')

    self.pool_stride = kwargs.get('pool_stride')

    self.feature_convs = FeatureConvs(self.in_channels, self.out_channels, self.kernel_size, self.padding, self.dropout)

  def forward(self, x):
    if self.layer != 0:
      x = F.max_pool2d(x, kernel_size=self.pool_stride)

    return self.feature_convs(x)


class Decoder(nn.Module):
  def __init__(self, layer, kwargs):
    super().__init__()
    self.layer = layer
    self.layers = kwargs.get('layers')

    self.in_channels = kwargs.get('out_channels')[layer]
    self.out_channels = kwargs.get('in_channels')[layer]
    self.kernel_size = kwargs.get('kernel_size')
    self.padding = kwargs.get('padding')

    self.pool_stride = kwargs.get('pool_stride')
    self.pointwise = nn.Conv2d(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1)

    self.feature_convs = FeatureConvs(self.in_channels, self.out_channels, self.kernel_size, self.padding)

  def forward(self, x, _x=None):
    if self.layer != self.layers - 1:
      x = self.pointwise(F.interpolate(x, scale_factor=self.pool_stride))
      x += _x

    return self.feature_convs(x)


class Bottleneck(nn.Module):
  def __init__(self, kwargs):
    super().__init__()

    self.dim_list = kwargs.get('mlp_dims')

    self.mlp = nn.ModuleList([MLPBlock(i, o) for i, o in zip(self.dim_list[:-1], self.dim_list[1:])])

  def forward(self, x):
    x = x.permute(0, 2, 3, 1)

    for i in self.mlp:
      x = i(x)
    return x.permute(0, 3, 1, 2)

class Light_UNet(nn.Module):
  def __init__(self, **kwargs):
    super().__init__()

    self.enc_1 = Encoder(0, kwargs)
    self.enc_2 = Encoder(1, kwargs)
    self.enc_3 = Encoder(2, kwargs)
    self.enc_4 = Encoder(3, kwargs)

    self.dec_3 = Decoder(2, kwargs)
    self.dec_2 = Decoder(1, kwargs)
    self.dec_1 = Decoder(0, kwargs)

    self.bottleneck = Bottleneck(kwargs)

    self.first_conv = nn.Sequential([
        nn.Conv2d(kwargs.get('feature_labels')[0], kwargs.get('in_channels')[0], kwargs.get('kernel_size'), kwargs.get('padding'), bias=False),
        nn.BatchNorm2d(num_features=kwargs.get('in_channels')[0]),
        nn.ReLU()
    ])

    self.last_conv = nn.Conv2d(kwargs.get('out_channels')[-1], kwargs.get('feature_labels')[-1], 1)
  
  def forward(self, x):
    x_1 = self.enc_1(self.first_conv(x))
    x_2 = self.enc_2(x_1)
    x_3 = self.enc_3(x_2)
    x_4 = self.bottleneck(self.enc_4(x_3))

    x_3 = self.dec_3(x_4, x_3)
    x_2 = self.dec_2(x_3, x_2)
    x_1 = self.dec_1(x_2, x_1)

    return self.last_conv(x_1)

config = {
    'layers': 4,
    'kernel_size': 3,
    'padding': 'same',
    'dropout': 0.1,
    'pool_stride': 2,
    'feature_labels': [12, 20], # Adjust element 1 based on number of valid labels for task.
    'in_channels': [64, 64, 128, 256],
    'out_channels': [64, 128, 256, 512],
    'mlp_dims': [512, 1024, 512]
}
